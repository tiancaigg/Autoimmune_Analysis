{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491da3e5-07da-4770-92c8-728a39cac24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "from os.path import exists\n",
    "import re\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5669e04b-fb64-4faf-bfbe-9bc2c71fcf0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c530b77-0f0f-4273-9e9b-e25b48e481cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "/media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif Â» nohup fimo --oc fimo_single_thread Ath_TF_binding_motifs_plantTFDB.meme /media/HDD2/Genomes/Ath_Ensembl56/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d156b8-d08a-4102-8858-481e179f910d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619 motifs were split into individual files.\n"
     ]
    }
   ],
   "source": [
    "from Bio import motifs\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the motifs from the MEME file\n",
    "with open('./scan_motif/Ath_TF_binding_motifs_plantTFDB.meme') as f:\n",
    "    meme_content = f.read()\n",
    "    # Splitting the file content by the 'MOTIF' keyword, but keeping the 'MOTIF' part\n",
    "    motifs_blocks = re.split(r'(?=MOTIF)', meme_content)[1:]  # Ignore the header before the first 'MOTIF'\n",
    "\n",
    "# Directory where the individual motif files will be saved\n",
    "output_dir = Path('./scan_motif/individual_motifs')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write each motif to a separate file named after the motif identifier\n",
    "for motif_block in motifs_blocks:\n",
    "    motif_id = re.search(r'MOTIF\\s+(\\w+)', motif_block).group(1)\n",
    "    motif_file = output_dir / f'{motif_id}.meme'\n",
    "    with open(motif_file, 'w') as f:\n",
    "        # Write the motif block to file\n",
    "        f.write(meme_content[:meme_content.index('MOTIF')] + motif_block.strip())  # Include header and motif\n",
    "\n",
    "print(f'{len(motifs_blocks)} motifs were split into individual files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398b10e0-8388-4a68-aa4d-ce5bbbd7333b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to run FIMO for a given motif file\n",
    "def run_fimo(motif_file):\n",
    "    # Extract the motif identifier from the file name\n",
    "    motif_id = motif_file.stem\n",
    "    # Define the output directory for this motif's FIMO results\n",
    "    output_dir = Path('./scan_motif/individual_fimo_scan') / motif_id\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define the log file path\n",
    "    log_file_path = output_dir / 'fimo.log'\n",
    "    \n",
    "    # Open the log file\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        # Build the FIMO command\n",
    "        cmd = [\n",
    "            'fimo',\n",
    "            '--oc', str(output_dir),\n",
    "            str(motif_file),\n",
    "            '/media/HDD2/Genomes/Ath_Ensembl56/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa'\n",
    "        ]\n",
    "        \n",
    "        # Run the command and redirect stdout and stderr to the log file\n",
    "        subprocess.run(cmd, stdout=log_file, stderr=subprocess.STDOUT, check=True)\n",
    "\n",
    "# Directory where the individual motif files are saved\n",
    "motif_dir = Path('./scan_motif/individual_motifs')\n",
    "motif_files = list(motif_dir.glob('*.meme'))\n",
    "\n",
    "# Set up the pool of workers and run FIMO on each motif file\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=120) as pool:  # Adjust the number of processes as needed\n",
    "        pool.map(run_fimo, motif_files)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4e29f79-9270-4f44-853e-c6c0b2fe3307",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the base directories\n",
    "fimo_scan_dir = Path('/media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan')\n",
    "output_dir = Path('./scan_motif/PWMscan_fimo_ath')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Glob all the fimo.tsv files from the individual_fimo_scan directories\n",
    "tsv_files = fimo_scan_dir.glob('**/fimo.tsv')\n",
    "\n",
    "# ...\n",
    "\n",
    "# Process each TSV file and convert it to BED format\n",
    "for tsv_file in tsv_files:\n",
    "    # Construct the output file path\n",
    "    bed_file_name = tsv_file.parent.name + '_TFBS.bed'\n",
    "    output_file_path = output_dir / bed_file_name\n",
    "    \n",
    "    # Open the TSV file and output file\n",
    "    with open(tsv_file, 'r') as f, open(output_file_path, 'w') as out_f:\n",
    "        # Skip the header line\n",
    "        next(f)\n",
    "        # Process each line in the TSV file\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            # Check if the line has the correct number of columns (at least 10)\n",
    "            if len(parts) < 10:\n",
    "                continue  # Skip lines that don't have enough columns\n",
    "            # Construct the BED format line\n",
    "            # BED format: chrom, chromStart, chromEnd, name, score, strand\n",
    "            # chrom = f'chr{parts[2]}'  # Prepend 'chr' assuming the sequence_name is a chromosome number\n",
    "            chrom = f'{parts[2]}'  # Prepend 'chr' assuming the sequence_name is a chromosome number\n",
    "            start = str(int(parts[3]) - 1)  # Convert to 0-based start position for BED format\n",
    "            end = parts[4]\n",
    "            name = parts[0]\n",
    "            score = parts[6]\n",
    "            strand = parts[5]\n",
    "            bed_line = f'{chrom}\\t{start}\\t{end}\\t{name}\\t{score}\\t{strand}\\n'\n",
    "            # Write to the output file\n",
    "            out_f.write(bed_line)\n",
    "\n",
    "\n",
    "print('Conversion to BED format completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b82795f7-4902-4dd1-9225-b1dfb99e91d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf ./scan_motif/PWMscan_fimo_ath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90f2f64a-5e50-4e29-983c-8f3541e686fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid entries for /media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan/AT5G25475/fimo.tsv. No BED file created.\n",
      "No valid entries for /media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan/AT5G48670/fimo.tsv. No BED file created.\n",
      "No valid entries for /media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan/AT5G58620/fimo.tsv. No BED file created.\n",
      "No valid entries for /media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan/AT1G60920/fimo.tsv. No BED file created.\n",
      "Conversion to BED format completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to convert a TSV file to a BED file\n",
    "def convert_tsv_to_bed(tsv_file, output_dir):\n",
    "    bed_file_name = tsv_file.parent.name + '_TFBS.bed'\n",
    "    output_file_path = output_dir / bed_file_name\n",
    "    \n",
    "    lines_written = False  # Flag to check if any line is written to the file\n",
    "\n",
    "    with open(tsv_file, 'r') as f:\n",
    "        next(f)  # Skip the header line\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 10:\n",
    "                continue  # Skip lines that don't have enough columns\n",
    "            chrom = f'{parts[2]}'\n",
    "            start = str(int(parts[3]) - 1)  # Convert to 0-based start position\n",
    "            end = parts[4]\n",
    "            # name = parts[0]\n",
    "            matched_seq = parts[9]\n",
    "            score = parts[6]\n",
    "            strand = parts[5]\n",
    "            bed_line = f'{chrom}\\t{start}\\t{end}\\t{matched_seq}\\t{score}\\t{strand}\\n'\n",
    "            lines.append(bed_line)\n",
    "            lines_written = True\n",
    "    \n",
    "    # Write to the output file only if there are lines to write\n",
    "    if lines_written:\n",
    "        with open(output_file_path, 'w') as out_f:\n",
    "            for line in lines:\n",
    "                out_f.write(line)\n",
    "    else:\n",
    "        print(f'No valid entries for {tsv_file}. No BED file created.')\n",
    "\n",
    "# Set the base directories\n",
    "fimo_scan_dir = Path('/media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan')\n",
    "output_dir = Path('./scan_motif/PWMscan_fimo_ath')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Glob all the fimo.tsv files\n",
    "tsv_files = list(fimo_scan_dir.glob('**/fimo.tsv'))\n",
    "\n",
    "# Process the TSV files in parallel\n",
    "with ThreadPoolExecutor(10) as executor:\n",
    "    # Using a lambda to pass the 'output_dir' to the function\n",
    "    executor.map(lambda tsv_file: convert_tsv_to_bed(tsv_file, output_dir), tsv_files)\n",
    "\n",
    "print('Conversion to BED format completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7bff010f-cfb7-4883-91cf-24b0a85e5e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t26000638\t26000659\tTTGCTTGTGTCTCACGTTACC\t23.8333\t-\n",
      "4\t2776284\t2776305\tTTGCTTGCTCTTCACGCTACC\t23.7576\t-\n",
      "5\t19151535\t19151556\tTTGCTTGTTGATCACGTTACC\t23\t-\n",
      "1\t22529499\t22529520\tTTGCTTGGGTTTCAAGCAACC\t22.8939\t+\n",
      "3\t18636363\t18636384\tTTGCTTGTTATACAAGTTACC\t22.4091\t+\n",
      "5\t15789957\t15789978\tTTGCTTGCTTCTTACGCTTCC\t22.3485\t-\n",
      "5\t15792242\t15792263\tTTGCTTGCTTCTTACGCTTCC\t22.3485\t-\n",
      "5\t15803294\t15803315\tTTGCTTGCTTCTTACGCTTCC\t22.3485\t-\n",
      "5\t23372567\t23372588\tTTGCTTGTGTTGTACGTAACA\t22.3485\t-\n",
      "4\t8583979\t8584000\tTTGCTTGTTGTTTACGTTTTC\t22.303\t-\n"
     ]
    }
   ],
   "source": [
    "! head ./scan_motif/PWMscan_fimo_ath/AT5G62380_TFBS.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b53c7d-e168-49f4-984b-54de411cf7de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31310492-bf2a-4f87-a348-56d5fdbaca34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7fb43ab0-52b7-412d-a74f-350a0977e964",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modification of BED files completed.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Function to add 'chr' prefix to the chromosome name in the BED files\n",
    "def add_chr_prefix_to_bed(bed_file, input_dir, output_dir):\n",
    "    modified_lines = []  # List to store modified lines\n",
    "    with open(input_dir / bed_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            parts[0] = 'chr' + parts[0]  # Add 'chr' prefix to the chromosome name\n",
    "            modified_lines.append('\\t'.join(parts) + '\\n')\n",
    "    \n",
    "    # Write the modified lines to the new file in the output directory\n",
    "    with open(output_dir / bed_file, 'w') as out_f:\n",
    "        for line in modified_lines:\n",
    "            out_f.write(line)\n",
    "\n",
    "# Set the base directories\n",
    "input_dir = Path('./scan_motif/PWMscan_fimo_ath')\n",
    "output_dir = Path('/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/PWMscan_fimo_ath_modified')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Get all the BED files from the input directory\n",
    "bed_files = list(input_dir.glob('*.bed'))\n",
    "\n",
    "# Process the BED files in parallel to add 'chr' prefix\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    # Using a lambda to pass both 'input_dir' and 'output_dir' to the function\n",
    "    executor.map(lambda bed_file: add_chr_prefix_to_bed(bed_file.name, input_dir, output_dir), bed_files)\n",
    "\n",
    "print('Modification of BED files completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8e6e6-65b4-4ef0-b31f-425bd3e330ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14f6c5d-0e30-4c9e-835e-64e27fdb5abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "aae289c1-3aa1-4f6f-b33e-574db0f1d51d",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "from pathlib import Path\n",
    "\n",
    "# Function to run FIMO for a given motif file\n",
    "def run_fimo(motif_file):\n",
    "    # Extract the motif identifier from the file name\n",
    "    motif_id = motif_file.stem\n",
    "    # Define the output directory for this motif's FIMO results\n",
    "    output_dir = Path('./scan_motif/individual_fimo_scan') / motif_id\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Define the log file path\n",
    "    log_file_path = output_dir / 'fimo.log'\n",
    "    \n",
    "    # Open the log file\n",
    "    with open(log_file_path, 'w') as log_file:\n",
    "        # Build the FIMO command\n",
    "        cmd = [\n",
    "            'fimo',\n",
    "            '--oc', str(output_dir),\n",
    "            str(motif_file),\n",
    "            '/media/HDD2/Genomes/Ath_Ensembl56/Arabidopsis_thaliana.TAIR10.dna.toplevel.fa'\n",
    "        ]\n",
    "        \n",
    "        # Run the command and redirect stdout and stderr to the log file\n",
    "        subprocess.run(cmd, stdout=log_file, stderr=subprocess.STDOUT, check=True)\n",
    "\n",
    "# Directory where the individual motif files are saved\n",
    "motif_dir = Path('./scan_motif/individual_motifs')\n",
    "motif_files = list(motif_dir.glob('*.meme'))\n",
    "\n",
    "# Set up the pool of workers and run FIMO on each motif file\n",
    "if __name__ == '__main__':\n",
    "    with Pool(processes=120) as pool:  # Adjust the number of processes as needed\n",
    "        pool.map(run_fimo, motif_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d4994a-b465-4597-ae33-dbde93f431b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! rm -rf ./scan_motif/PWMscan_fimo_ath"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f195393-789f-412d-9223-2781176e1efd",
   "metadata": {
    "tags": []
   },
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Set the base directories\n",
    "fimo_scan_dir = Path('/media/HDD2/donghui/bulk_ATAC_DM1_DM2d/scan_motif/individual_fimo_scan')\n",
    "output_dir = Path('./scan_motif/PWMscan_fimo_ath')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Glob all the fimo.tsv files from the individual_fimo_scan directories\n",
    "tsv_files = fimo_scan_dir.glob('**/fimo.tsv')\n",
    "\n",
    "# ...\n",
    "\n",
    "# Process each TSV file and convert it to BED format\n",
    "for tsv_file in tsv_files:\n",
    "    # Construct the output file path\n",
    "    bed_file_name = tsv_file.parent.name + '_TFBS.bed'\n",
    "    output_file_path = output_dir / bed_file_name\n",
    "    \n",
    "    # Open the TSV file and output file\n",
    "    with open(tsv_file, 'r') as f, open(output_file_path, 'w') as out_f:\n",
    "        # Skip the header line\n",
    "        next(f)\n",
    "        # Process each line in the TSV file\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            # Check if the line has the correct number of columns (at least 10)\n",
    "            if len(parts) < 10:\n",
    "                continue  # Skip lines that don't have enough columns\n",
    "            # Construct the BED format line\n",
    "            # BED format: chrom, chromStart, chromEnd, name, score, strand\n",
    "            # chrom = f'chr{parts[2]}'  # Prepend 'chr' assuming the sequence_name is a chromosome number\n",
    "            chrom = f'{parts[2]}'  # Prepend 'chr' assuming the sequence_name is a chromosome number\n",
    "            start = str(int(parts[3]) - 1)  # Convert to 0-based start position for BED format\n",
    "            end = parts[4]\n",
    "            name = parts[0]\n",
    "            score = parts[6]\n",
    "            strand = parts[5]\n",
    "            bed_line = f'{chrom}\\t{start}\\t{end}\\t{name}\\t{score}\\t{strand}\\n'\n",
    "            # Write to the output file\n",
    "            out_f.write(bed_line)\n",
    "\n",
    "\n",
    "print('Conversion to BED format completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f87f3-e0cf-4642-ac2a-a3f27b6f8eba",
   "metadata": {},
   "source": [
    "## Modify narrowPeak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1d0d028-5605-4803-94bf-7640f61ead53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modification of narrowPeak files completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import glob\n",
    "\n",
    "def process_narrowPeak_file(file_path, output_dir):\n",
    "    # Read the narrowPeak file using pandas\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    \n",
    "    # Modify the first column by adding 'chr' prefix\n",
    "    df[0] = 'chr' + df[0].astype(str)\n",
    "    \n",
    "    # Modify the fourth column by removing the directory path\n",
    "    df[3] = df[3].apply(lambda x: os.path.basename(x))\n",
    "    \n",
    "    # Construct the output file path\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    \n",
    "    # Write the modified DataFrame to the new file\n",
    "    df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "# Set the directories\n",
    "input_dir = Path('/media/HDD3/bulk_ATAC_DMs/DM1_DM2d_Novogene/X401SC23101639-Z01-F001/Analysi_dir/5_macs2peaks_simple')\n",
    "output_dir = '/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/data/5_macs2peaks_modified'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# Glob all the narrowPeak files\n",
    "narrowPeak_files = glob.glob(str(input_dir / '*.narrowPeak'))\n",
    "\n",
    "# Process the narrowPeak files in parallel\n",
    "with ThreadPoolExecutor(max_workers = 20) as executor:\n",
    "    for file_path in narrowPeak_files:\n",
    "        executor.submit(process_narrowPeak_file, file_path, output_dir)\n",
    "\n",
    "print('Modification of narrowPeak files completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07e8a360-7454-4382-8ac5-6779641a5885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modification of narrowPeak files completed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import glob\n",
    "\n",
    "def process_narrowPeak_file(file_path, output_dir):\n",
    "    # Read the narrowPeak file using pandas\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    \n",
    "    # Modify the first column by adding 'chr' prefix\n",
    "    df[0] = 'chr' + df[0].astype(str)\n",
    "    \n",
    "    # Modify the fourth column by removing the directory path\n",
    "    df[3] = df[3].apply(lambda x: os.path.basename(x))\n",
    "    \n",
    "    # Construct the output file path\n",
    "    output_file_path = os.path.join(output_dir, os.path.basename(file_path))\n",
    "    \n",
    "    # Write the modified DataFrame to the new file\n",
    "    df.to_csv(output_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "# Set the directories, DM6-DM7\n",
    "input_dir = Path('/media/HDD3/bulk_ATAC_DMs/N2315964_30-927980316_Lib_2023-09-30/Analysi_dir/5_macs2peaks_simple')\n",
    "output_dir = '/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/data/5_macs2peaks_modified'\n",
    "os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# Glob all the narrowPeak files\n",
    "narrowPeak_files = glob.glob(str(input_dir / '*.narrowPeak'))\n",
    "\n",
    "# Process the narrowPeak files in parallel\n",
    "with ThreadPoolExecutor(max_workers = 20) as executor:\n",
    "    for file_path in narrowPeak_files:\n",
    "        executor.submit(process_narrowPeak_file, file_path, output_dir)\n",
    "\n",
    "print('Modification of narrowPeak files completed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b371db5f-2695-47c1-a323-9d1bc35cfa4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "306fbaf9-1e05-4c60-8257-ef0fe99fb4b1",
   "metadata": {},
   "source": [
    "## the TF table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad9a08a2-2782-413b-9b97-60d6780627e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../gene_table_and_other_scripts/bioMartR_Ath_all_genes_info_googled.csv')\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={'ensembl_gene_id': 'ENSEMBL', 'external_gene_name': 'SYMBOL'})\n",
    "df['SYMBOL'] = df['SYMBOL'].fillna(df['ENSEMBL'])\n",
    "# Create new column HOCOID same as SYMBOL\n",
    "df['HOCOID'] = df['SYMBOL']\n",
    "\n",
    "# Order the columns as SYMBOL, ENSEMBL, HOCOID\n",
    "df = df[['SYMBOL', 'ENSEMBL', 'HOCOID']]\n",
    "df = df.sort_values('ENSEMBL')\n",
    "df = df.reset_index(drop=True)\n",
    "# Save the DataFrame to a new CSV file without index\n",
    "df.to_csv('/media/HDD2/donghui/diffTF_ath/diffTF/src/TF_Gene_TranslationTables/df_translationTable_ath.csv', \n",
    "          sep='\\t', index=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b3ee84-7a5b-4169-9946-5397133f00f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce4cbd4b-ccad-4b8f-86ff-054de5ce66b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change dir to /media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/referenceGenome\n",
    "os.chdir('/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/referenceGenome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "69d10e6e-e89f-4f9e-bfad-31161fd2ee2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabidopsis_thaliana.TAIR10.dna.toplevel.fa']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##list the files in the directory\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe4e9c-d817-4fc6-a3e1-ab1e08d95515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chr prefix to each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70296a95-3385-47aa-baa4-961707b291d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! sed 's/>\\(.*\\)/>chr\\1/' Arabidopsis_thaliana.TAIR10.dna.toplevel.fa > Arabidopsis_thaliana.TAIR10.modified.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c22ea-a544-4eb0-bd34-357982c5aec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f192a655-f9fd-4403-b24d-6ad4aaf4f89c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/tmp/ipykernel_2409767/437056502.py\", line 8, in modify_chromosome_names\n    df = pd.read_csv(file_path, sep='\\t', header=None)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 948, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1448, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1723, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 93, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"parsers.pyx\", line 586, in pandas._libs.parsers.TextReader.__cinit__\npandas.errors.EmptyDataError: No columns to parse from file\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m         pool\u001b[38;5;241m.\u001b[39mstarmap(modify_chromosome_names, [(file, target_dir) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Use a Pool to process files in parallel\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodify_chromosome_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to modify the chromosome names\n",
    "def modify_chromosome_names(file_path, target_dir):\n",
    "    # Read the file\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "    # Modify the first column, assuming it contains the chromosome names\n",
    "    df[0] = 'chr' + df[0].astype(str)\n",
    "\n",
    "    # Create a new file path\n",
    "    new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Write the modified DataFrame to the new file path\n",
    "    df.to_csv(new_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    # Define the source and target directories\n",
    "    source_dir = '/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/PWMscan_fimo_ath'\n",
    "    target_dir = '/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/PWMscan_fimo_ath_modified'\n",
    "\n",
    "    # Create the target directory if it does not exist\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Get all .bed files in the source directory\n",
    "    files = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if f.endswith('.bed')]\n",
    "\n",
    "    # Use a Pool to process files in parallel\n",
    "    with Pool(10) as pool:\n",
    "        pool.starmap(modify_chromosome_names, [(file, target_dir) for file in files])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3a0d529-60f4-4575-965a-856eb4325c67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SYMBOL    ENSEMBL     HOCOID\n",
      "0      NAC001  AT1G01010  AT1G01010\n",
      "1        ARV1  AT1G01020  AT1G01020\n",
      "2        NGA3  AT1G01030  AT1G01030\n",
      "3        DCL1  AT1G01040  AT1G01040\n",
      "4  ath-MIR838  AT1G01046  AT1G01046\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('../gene_table_and_other_scripts/bioMartR_Ath_all_genes_info_googled.csv')\n",
    "\n",
    "# Rename columns\n",
    "df = df.rename(columns={'ensembl_gene_id': 'ENSEMBL', 'external_gene_name': 'SYMBOL'})\n",
    "df['SYMBOL'] = df['SYMBOL'].str.replace(' ', '_')\n",
    "df['SYMBOL'] = df['SYMBOL'].str.replace('\"', '')\n",
    "df['SYMBOL'] = df['SYMBOL'].fillna(df['ENSEMBL'])\n",
    "# Create new column HOCOID same as SYMBOL\n",
    "df['HOCOID'] = df['ENSEMBL']\n",
    "\n",
    "\n",
    "# Order the columns as SYMBOL, ENSEMBL, HOCOID\n",
    "df = df[['SYMBOL', 'ENSEMBL', 'HOCOID']]\n",
    "df = df.sort_values('ENSEMBL')\n",
    "df = df.reset_index(drop=True)\n",
    "# Save the DataFrame to a new CSV file without index\n",
    "print(df.head())\n",
    "df.to_csv('/media/HDD2/donghui/diffTF_ath/diffTF/src/TF_Gene_TranslationTables/df_translationTable_ath2.csv', \n",
    "          sep=' ', index=False,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048960f-4717-47b1-be56-b3aa26cbdb8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f7464f0-e019-4008-a9e5-0737d80981f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change dir to /media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/referenceGenome\n",
    "os.chdir('/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/referenceGenome')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d9648a0-5045-427e-8f8c-bc2e76f6d3a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabidopsis_thaliana.TAIR10.dna.toplevel.fa']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##list the files in the directory\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcac6af-fc03-4f60-bd46-b48e3a328c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add chr prefix to each chromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98990630-f515-4952-b6ff-bd152133dc28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! sed 's/>\\(.*\\)/>chr\\1/' Arabidopsis_thaliana.TAIR10.dna.toplevel.fa > Arabidopsis_thaliana.TAIR10.modified.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e773e5-27da-4819-9abf-698e76497d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a29f112-0a69-4a25-bfe8-492f860dff75",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/tmp/ipykernel_2409767/437056502.py\", line 8, in modify_chromosome_names\n    df = pd.read_csv(file_path, sep='\\t', header=None)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 948, in read_csv\n    return _read(filepath_or_buffer, kwds)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 611, in _read\n    parser = TextFileReader(filepath_or_buffer, **kwds)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1448, in __init__\n    self._engine = self._make_engine(f, self.engine)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/readers.py\", line 1723, in _make_engine\n    return mapping[engine](f, **self.options)\n  File \"/home/hu/miniconda3/envs/work2/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py\", line 93, in __init__\n    self._reader = parsers.TextReader(src, **kwds)\n  File \"parsers.pyx\", line 586, in pandas._libs.parsers.TextReader.__cinit__\npandas.errors.EmptyDataError: No columns to parse from file\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m         pool\u001b[38;5;241m.\u001b[39mstarmap(modify_chromosome_names, [(file, target_dir) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files])\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 36\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Use a Pool to process files in parallel\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Pool(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodify_chromosome_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/work2/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Function to modify the chromosome names\n",
    "def modify_chromosome_names(file_path, target_dir):\n",
    "    # Read the file\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "    # Modify the first column, assuming it contains the chromosome names\n",
    "    df[0] = 'chr' + df[0].astype(str)\n",
    "\n",
    "    # Create a new file path\n",
    "    new_file_path = os.path.join(target_dir, os.path.basename(file_path))\n",
    "\n",
    "    # Write the modified DataFrame to the new file path\n",
    "    df.to_csv(new_file_path, sep='\\t', header=False, index=False)\n",
    "\n",
    "def main():\n",
    "    # Define the source and target directories\n",
    "    source_dir = '/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/PWMscan_fimo_ath'\n",
    "    target_dir = '/media/HDD2/donghui/diffTF_ath/diffTF/example/stable/input/PWMscan_fimo_ath_modified'\n",
    "\n",
    "    # Create the target directory if it does not exist\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    # Get all .bed files in the source directory\n",
    "    files = [os.path.join(source_dir, f) for f in os.listdir(source_dir) if f.endswith('.bed')]\n",
    "\n",
    "    # Use a Pool to process files in parallel\n",
    "    with Pool(10) as pool:\n",
    "        pool.starmap(modify_chromosome_names, [(file, target_dir) for file in files])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:work2] *",
   "language": "python",
   "name": "conda-env-work2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
